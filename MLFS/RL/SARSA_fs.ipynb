{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA Function iterative step:   \n",
    "  \n",
    "$ \\large s_t $ : state at time t   \n",
    "$ \\large a_t $ : action at time t    \n",
    "$ \\large r_t $ : reward at time t  \n",
    "$ \\large \\alpha $ : learning rate. Determines to what extent newly acquired information overrides old information      \n",
    "$ \\large \\gamma $ : discount factor. Determines the importance of future rewards (0 is short-sighted, 1 long-sighted)   \n",
    "  \n",
    "$ \\large Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\; [\\; r_{t+1} + \\gamma \\; Q(s_{t+1}, a_{t+1}) - Q(s_{t}, a_{t}) ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((16,4))\n",
    "nextState = 0\n",
    "episode = 1\n",
    "Path = []\n",
    "\n",
    "def SelectAction(cState, nState):\n",
    "    i = random.random()\n",
    "    if(i > epsilon):\n",
    "        MaxRew = [-9, -9, -9, -9]\n",
    "        for j in action[cState]: MaxRew[j] = Q[nState][j]\n",
    "        cAct = np.argmax(MaxRew)\n",
    "    else:\n",
    "        cAct = random.choice(action[cState])\n",
    "    return cAct\n",
    "\n",
    "for i in range(10):\n",
    "    re = 0\n",
    "    currState = 0\n",
    "    Path = []\n",
    "    currAct = SelectAction(currState, nextState)\n",
    "\n",
    "    while(currState != goalState):\n",
    "        nextState = States[currState][currAct]\n",
    "        nextAct = SelectAction(nextState, nextState)\n",
    "\n",
    "        Q[currState][currAct] = Q[currState][currAct] + lrA * (reward[currState][currAct] + gamma * Q[nextState][nextAct] - Q[currState][currAct])\n",
    "        Path.append(currState)\n",
    "        currState = nextState\n",
    "        currAct = nextAct\n",
    "    \n",
    "    Path.append(currState)\n",
    "    episode += 1\n",
    "    if epsilon > 0: epsilon -= decEpsilon\n",
    "\n",
    "print(Path)\n",
    "print(\"The Final Q Matrix is: \\n\", np.divide(Q,np.amax(Q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q Learning\n",
      "\n",
      "[0, 4, 5, 9, 10, 11, 7]\n",
      "The Final Q Matrix is: \n",
      " [[ 0.          0.01725053  0.         -0.07000041]\n",
      " [ 0.         -0.00700004 -0.01043006  0.        ]\n",
      " [ 0.         -0.07000041  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.02398305 -0.02244934  0.          0.14180793]\n",
      " [-0.07000041  0.30064475 -0.02214583 -0.07000041]\n",
      " [-0.00700004 -0.00700004 -0.01043006  0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.02255942 -0.02074328  0.         -0.01830203]\n",
      " [-0.01355908 -0.01540709 -0.01805652  0.47060316]\n",
      " [-0.07000041 -0.01253007 -0.01043006  0.68976162]\n",
      " [ 1.          0.          0.          0.        ]\n",
      " [-0.01740301  0.          0.          0.03141913]\n",
      " [ 0.22769324  0.         -0.01253007 -0.01316008]\n",
      " [-0.01253007  0.         -0.01043006  0.15554092]\n",
      " [ 0.57057337  0.          0.          0.        ]]\n",
      "\n",
      "SARSA Learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reward = np.array([[0, -1, 0, -10],\n",
    "                   [0, -1, -1, -1],\n",
    "                   [0, -10, -10, -1],\n",
    "                   [0, 100, -1, 0],\n",
    "                   \n",
    "                   [-1, -1, 0, -1],\n",
    "                   [-10, -1, -1, -10],\n",
    "                   [-1, -1, -1, 100],\n",
    "                   [-1, -1, -1, 0],\n",
    "                   \n",
    "                   [-1, -1, 0, -1],\n",
    "                   [-1, -1, -1, -1],\n",
    "                   [-10, -1, -1, -1],\n",
    "                   [100, -1, -1, 0],\n",
    "                   \n",
    "                   [-1, 0, 0, -1],\n",
    "                   [-1, 0, -1, -1],\n",
    "                   [-1, 0, -1, -1],\n",
    "                   [-1, 0, -1, 0]])\n",
    "\n",
    "States = np.array([[-1,4,-1,1],\n",
    "                [-1,5,0,2],\n",
    "                [-1,6,1,3],\n",
    "                [-1,7,2,-1],\n",
    "                \n",
    "                [0,8,-1,5],\n",
    "                [1,9,4,6],\n",
    "                [2,10,5,7],\n",
    "                [3,11,6,-1],\n",
    "                \n",
    "                [4,12,-1,9],\n",
    "                [5,13,8,10],\n",
    "                [6,14,9,11],\n",
    "                [7,15,10,-1],\n",
    "                \n",
    "                [8,-1,-1,13],\n",
    "                [9,-1,12,14],\n",
    "                [10,-1,13,15],\n",
    "                [11,-1,14,-1]])\n",
    "\n",
    "action = np.array([[1,3],\n",
    "                   [1, 2, 3],\n",
    "                   [1, 2, 3],\n",
    "                   [1, 2],\n",
    "                   \n",
    "                   [0,1,3],\n",
    "                   [0,1,2,3],\n",
    "                   [0,1,2,3],\n",
    "                   [0,1,2],\n",
    "                   \n",
    "                   [0,1,3],\n",
    "                   [0,1,2,3],\n",
    "                   [0,1,2,3],\n",
    "                   [0,1,2],\n",
    "                   \n",
    "                   [0,3],\n",
    "                   [0,2,3],\n",
    "                   [0,2,3],\n",
    "                   [0,2]])\n",
    "\n",
    "\n",
    "epsilon = 0.3\n",
    "decEpsilon = 0.03\n",
    "gamma = 0.7\n",
    "lrA = 0.7\n",
    "goalState = 7\n",
    "\n",
    "print(\"\\nQ Learning\\n\")\n",
    "\n",
    "Q = np.zeros((16,4))\n",
    "nextState = 0\n",
    "episode = 1\n",
    "Path = []\n",
    "\n",
    "for i in range(10):\n",
    "    re = 0\n",
    "    currState = 0\n",
    "    Path = []\n",
    "\n",
    "    while(currState != 7):\n",
    "        Qx = -999\n",
    "        for i in action[currState]:\n",
    "            if Q[currState][i] > Qx:\n",
    "                currAct = i\n",
    "                Qx = Q[currState][i]\n",
    "        nextState = States[currState][currAct]\n",
    "        \"print(i_state,\"  \",n_state)\"\n",
    "        \n",
    "        nextValues = []\n",
    "        for i in action[nextState]:\n",
    "            nextValues.append(Q[nextState][i])\n",
    "        Max = max(nextValues)\n",
    "        re = re + reward[currState][currAct] + gamma * Max - Q[currState][currAct]\n",
    "        Q[currState][currAct] = Q[currState][currAct] + lrA * (reward[currState][currAct] + gamma * Max - Q[currState][currAct])\n",
    "        Path.append(currState)\n",
    "        currState = nextState\n",
    "    Path.append(currState)\n",
    "    episode += 1\n",
    "\n",
    "print(Path)\n",
    "print(\"The Final Q Matrix is: \\n\", np.divide(Q,np.amax(Q)))\n",
    "\n",
    "print(\"\\nSARSA Learning\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
