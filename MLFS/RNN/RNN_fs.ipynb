{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Recurrent Neural Networks\n",
    "### From scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFNN\n",
    "\n",
    "$ \\large h_t = \\sigma_h(W_h x_t + b_h) $  \n",
    "$ \\large o_t = \\sigma_h(W_o h_t + b_o) $\n",
    "\n",
    "TDNN\n",
    "\n",
    "$ \\large h_t = \\sigma_h(W_h x_t + V x_{t-1} + b_h) $  \n",
    "$ \\large o_t = \\sigma_h(W_o h_t + b_o) $\n",
    "\n",
    "Elman RNN\n",
    "\n",
    "$ \\large h_t = \\sigma_h(W_h x_t + V h_{t-1} + b_h) $  \n",
    "$ \\large o_t = \\sigma_h(W_o h_t + b_o) $\n",
    "\n",
    "Jordan RNN\n",
    "\n",
    "$ \\large h_t = \\sigma_h(W_h x_t + V o_{t-1} + b_h) $  \n",
    "$ \\large o_t = \\sigma_h(W_o h_t + b_o) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import numpy as np\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Activation functions\n",
    "\n",
    "$ \\large s = \\frac{1}{1 + e^{-x}} \\qquad t = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "\n",
    "    def forward(self, x): \n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def backward(self, x, topDiff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - output) * output * topDiff\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, x, topDiff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - np.square(output)) * topDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGate:\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2\n",
    "\n",
    "    def backward(self, x1, x2, dz):\n",
    "        dx1 = dz * np.ones_like(x1)\n",
    "        dx2 = dz * np.ones_like(x2)\n",
    "        return dx1, dx2\n",
    "\n",
    "class MultiplyGate:\n",
    "\n",
    "    def forward(self,W, x):\n",
    "        return np.dot(W, x)\n",
    "\n",
    "    def backward(self, W, x, dz):\n",
    "        dW = np.asarray(np.dot(np.transpose(np.asmatrix(dz)), np.asmatrix(x)))\n",
    "        dx = np.dot(np.transpose(W), dz)\n",
    "        return dW, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mulGate = MultiplyGate()\n",
    "        self.addGate = AddGate()\n",
    "        self.activation = Tanh()\n",
    "\n",
    "    def forward(self, x, prev, U, W, V):\n",
    "        self.mulu = self.mulGate.forward(U, x)\n",
    "        self.mulw = self.mulGate.forward(W, prev)\n",
    "        self.add = self.addGate.forward(self.mulw, self.mulu)\n",
    "        self.s = self.activation.forward(self.add)\n",
    "        self.mulv = self.mulGate.forward(V, self.s)\n",
    "\n",
    "    def backward(self, x, prev, U, W, V, diff, dmulv):\n",
    "        self.forward(x, prev, U, W, V)\n",
    "        dV, dsv = self.mulGate.backward(V, self.s, dmulv)\n",
    "        ds = dsv + diff\n",
    "        dadd = self.activation.backward(self.add, ds)\n",
    "        dmulw, dmulu = self.addGate.backward(self.mulw, self.mulu, dadd)\n",
    "        dW, dprev = self.mulGate.backward(W, prev, dmulw)\n",
    "        dU, dx = self.mulGate.backward(U, x, dmulu)\n",
    "        return (dprev, dU, dW, dV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "\n",
    "    def predict(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[y])\n",
    "\n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        probs[y] -= 1.0\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, wordDim, nHid=100, bpttTrunc=4):\n",
    "        self.wordDim = wordDim\n",
    "        self.nHid = nHid\n",
    "        self.bpttTrunc = bpttTrunc\n",
    "        self.U = np.random.uniform(-np.sqrt(1. / wordDim), np.sqrt(1. / wordDim), (nHid, wordDim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1. / nHid), np.sqrt(1. / nHid), (nHid, nHid))\n",
    "        self.V = np.random.uniform(-np.sqrt(1. / nHid), np.sqrt(1. / nHid), (wordDim, nHid))\n",
    "     \n",
    "    # Forward propagation (predicting word probabilities) i.e. x = [0, 179, 341, 416], then its y = [179, 341, 416, 1]\n",
    "    def Forward(self, x):\n",
    "        T = len(x) # total number of time steps\n",
    "        layers = []\n",
    "        prev = np.zeros(self.nHid)\n",
    "        for t in range(T):\n",
    "            layer = Layer()\n",
    "            input = np.zeros(self.wordDim)\n",
    "            input[x[t]] = 1\n",
    "            layer.forward(input, prev, self.U, self.W, self.V)\n",
    "            prev = layer.s\n",
    "            layers.append(layer)\n",
    "        return layers\n",
    "\n",
    "    def Predict(self, x):\n",
    "        output = Softmax()\n",
    "        layers = self.Forward(x)\n",
    "        return [np.argmax(output.predict(layer.mulv)) for layer in layers]\n",
    "\n",
    "    def CalcLoss(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        output = Softmax()\n",
    "        layers = self.Forward(x)\n",
    "        loss = 0.0\n",
    "        for i, layer in enumerate(layers):\n",
    "            loss += output.loss(layer.mulv, y[i])\n",
    "        return loss / float(len(y))\n",
    "\n",
    "    def CalcTotalLoss(self, X, Y):\n",
    "        loss = 0.0\n",
    "        for i in range(len(Y)):\n",
    "            loss += self.CalcLoss(X[i], Y[i])\n",
    "        return loss / float(len(Y))\n",
    "\n",
    "    def Bptt(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        output = Softmax()\n",
    "        layers = self.Forward(x)\n",
    "        dU = np.zeros(self.U.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "\n",
    "        T = len(layers)\n",
    "        prevT = np.zeros(self.nHid)\n",
    "        diff = np.zeros(self.nHid)\n",
    "        for t in range(0, T):\n",
    "            dmulv = output.diff(layers[t].mulv, y[t])\n",
    "            input = np.zeros(self.wordDim)\n",
    "            input[x[t]] = 1\n",
    "            dprev, dU_t, dW_t, dV_t = layers[t].backward(input, prevT, self.U, self.W, self.V, diff, dmulv)\n",
    "            prevT = layers[t].s\n",
    "            dmulv = np.zeros(self.wordDim)\n",
    "            for i in range(t-1, max(-1, t-self.bpttTrunc-1), -1):\n",
    "                input = np.zeros(self.wordDim)\n",
    "                input[x[i]] = 1\n",
    "                prevI = np.zeros(self.nHid) if i == 0 else layers[i-1].s\n",
    "                dprev, dU_i, dW_i, dV_i = layers[i].backward(input, prevI, self.U, self.W, self.V, dprev, dmulv)\n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "            dV += dV_t\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "        return (dU, dW, dV)\n",
    "\n",
    "    def SgdStep(self, x, y, m):\n",
    "        dU, dW, dV = self.Bptt(x, y)\n",
    "        self.U -= m * dU\n",
    "        self.V -= m * dV\n",
    "        self.W -= m * dW\n",
    "\n",
    "    def Train(self, X, Y, m=0.005, epochs=100, evalLoss=5):\n",
    "        nExamples = 0\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            if (epoch % evalLoss == 0):\n",
    "                loss = self.CalcTotalLoss(X, Y)\n",
    "                losses.append((nExamples, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"%s: Loss after n examples = %d epoch=%d: %f\" % (time, nExamples, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                    m = m * 0.5\n",
    "                    print(\"Setting learning rate to %f\" % m)\n",
    "                sys.stdout.flush()\n",
    "            # For each training example...\n",
    "            for i in range(len(Y)):\n",
    "                self.SgdStep(X[i], Y[i], m)\n",
    "                nExamples += 1\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing : Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSentenceData(path, vocabDim=8000):\n",
    "    unknown_token = \"UNKNOWN_TOKEN\"\n",
    "    sentence_start_token = \"SENTENCE_START\"\n",
    "    sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "    # Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "    print(\"Reading CSV file...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, skipinitialspace=True)\n",
    "        # Split full comments into sentences\n",
    "        sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "        # Append SENTENCE_START and SENTENCE_END\n",
    "        sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "    print(\"Parsed %d sentences.\" % (len(sentences)))\n",
    "\n",
    "    # Tokenize the sentences into words\n",
    "    tokenizedSentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    # Filter the sentences having few words (including SENTENCE_START and SENTENCE_END)\n",
    "    tokenizedSentences = list(filter(lambda x: len(x) > 3, tokenizedSentences))\n",
    "\n",
    "    # Count the word frequencies\n",
    "    wordFreq = nltk.FreqDist(itertools.chain(*tokenizedSentences))\n",
    "    print(\"Found %d unique words tokens.\" % len(wordFreq.items()))\n",
    "\n",
    "    # Get the most common words and build index2word and word2index vectors\n",
    "    vocab = wordFreq.most_common(vocabDim-1)\n",
    "    index2word = [x[0] for x in vocab]\n",
    "    index2word.append(unknown_token)\n",
    "    word2index = dict([(w,i) for i,w in enumerate(index2word)])\n",
    "\n",
    "    print(\"Using vocabulary size %d.\" % vocabDim)\n",
    "    print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "    # Replace all words not in our vocabulary with the unknown token\n",
    "    for i, sent in enumerate(tokenizedSentences):\n",
    "        tokenizedSentences[i] = [w if w in word2index else unknown_token for w in sent]\n",
    "\n",
    "    print(\"\\nExample sentence: '%s'\" % sentences[1])\n",
    "    print(\"\\nExample sentence after Pre-processing: '%s'\\n\" % tokenizedSentences[0])\n",
    "\n",
    "    # Create the training data\n",
    "    XTrain = np.asarray([[word2index[w] for w in sent[:-1]] for sent in tokenizedSentences])\n",
    "    yTrain = np.asarray([[word2index[w] for w in sent[1:]] for sent in tokenizedSentences])\n",
    "\n",
    "    print(\"XTrain shape: \" + str(XTrain.shape))\n",
    "    print(\"yTrain shape: \" + str(yTrain.shape))\n",
    "\n",
    "    # Print a training data example\n",
    "    xExample, yExample = XTrain[17], yTrain[17]\n",
    "    print(\"x:\\n%s\\n%s\" % (\" \".join([index2word[x] for x in xExample]), xExample))\n",
    "    print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index2word[x] for x in yExample]), yExample))\n",
    "\n",
    "    return XTrain, yTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing : run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79171 sentences.\n",
      "Found 65409 unique words tokens.\n",
      "Using vocabulary size 1000.\n",
      "The least frequent word in our vocabulary is 'america' and appeared 129 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'UNKNOWN_TOKEN', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'UNKNOWN_TOKEN', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n",
      "\n",
      "XTrain shape: (78483,)\n",
      "yTrain shape: (78483,)\n",
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 51, 27, 16, 10, 858, 54, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[51, 27, 16, 10, 858, 54, 25, 34, 69, 1]\n",
      "2019-11-22 10:21:57: Loss after n examples = 0 epoch=0: 6.906628\n",
      "2019-11-22 10:22:10: Loss after n examples = 100 epoch=1: 6.807144\n",
      "2019-11-22 10:22:21: Loss after n examples = 200 epoch=2: 4.976012\n",
      "2019-11-22 10:22:33: Loss after n examples = 300 epoch=3: 4.794112\n",
      "2019-11-22 10:22:44: Loss after n examples = 400 epoch=4: 4.699667\n"
     ]
    }
   ],
   "source": [
    "wordDim = 1000\n",
    "nHid = 100\n",
    "XTrain, yTrain = GetSentenceData('D:/data/csv/reddit-comments.csv', wordDim)\n",
    "\n",
    "np.random.seed(10)\n",
    "rnn = RNN(wordDim, nHid)\n",
    "losses = rnn.Train(XTrain[:100], yTrain[:100], m=0.005, epochs=5, evalLoss=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
